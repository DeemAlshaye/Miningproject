---
title: "Data Salaries"
output:
  html_document:
    toc: true
    toc_depth: 2
---

## Goal of collecting this Dataset

Predicting Data Science job salaries using classification.

## The source of the dataset

Link to the source Dataset

<https://www.kaggle.com/datasets/arnabchaki/data-science-salaries-2023?resource=download>

## Information about the Dataset

Number of Attributes: 11

Type of Attributes: nominal, ordinal, numeric

Number of objects: 3755

Class name: salary in usd

| Attributes names   | Description                                                                                 | Data type        | Possible values                                                                                                                  |
|-----------------|-----------------|-----------------|---------------------|
| work_year          | The year the salary was paid                                                                | numirec          | Range between 2020-2023                                                                                                          |
| experience_level   | The experience level in the job during the year                                             | ordinal          | (SE:"Senior",MI:"Mid level",EN:"Entry level,EX:"Executive level")                                                                |
| employment_type    | The type of employment for the role                                                         | nominal          | (FT:"Full time",PT:"Part time",CT:"Contractual",FL:"Freelancer")                                                                 |
| job_title          | The role worked in during the year                                                          | nominal          | (Data Engineer, Data Scientist,Data Analyst, Machine Learning Engineer, Analytics Engineer...)It has 93 categories               |
| salary             | The total gross salary amount paid                                                          | numeric interval | Range Between [6000 - 30.4m]                                                                                                     |
| salary_currency    | The currency of the salary paid as an ISO 4217 currency code                                | nominal          | USD,EUR,GBP,INR,CAD...)                                                                                                          |
| salaryinusd        | The salary in USD                                                                           | numeric interval | Between[5132-450k]                                                                                                               |
| employee_residence | Employee's primary country of residence in during the work year as an ISO 3166 country code | nominal          | (US,GB,CA,ES,IN...)It has 78 categories                                                                                          |
| remote_ratio       | The overall amount of work done remotely                                                    | numeric Ratio    | (0, 50, 100)                                                                                                                     |
| company_location   | The country of the employer's main office or contracting branch                             | nominal          | (AE, AL, AM, AR. AS, AT, AU, BA BE, BO, BR, BS, CA, CF, CH, CL, CN, CO, CR, CZ, DE, DK, DZ, EE, EG,ES......)It has 72 categories |
| company_size       | The median number of people that worked for the company during the year                     | ordinal          | (S,M,L)                                                                                                                          |

## library used

```{r}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(magrittr)
library(rpart)
library(rpart.plot)
library(caret)
library(Rcpp)
library(e1071)
library(C50)
library(printr)


```

## uploading the data and raw samples

raw data samples

```{r}
ds_salaries= read.csv(url("https://raw.githubusercontent.com/DeemAlshaye/Miningproject/main/dataset/ds_salaries.csv"), header=TRUE)
head(ds_salaries)
tail(ds_salaries)
```

```{r}
dim(ds_salaries)
```

## summary the data

we used summary function to show each numeric value min, median, max, Q1 and Q3

```{r}
summary(ds_salaries)
```

## statical mesurment

Statistical measurements allows to explore our data in a quantitative manner,it helps to understand its distribution, variability, and key characteristics.

```{r}
mean(ds_salaries$salary)
median(ds_salaries$salary)
quantile(ds_salaries$salary)
```

we see that the mean is close to the median,and quantiles has high changes

```{r}
mean(ds_salaries$salary_in_usd)
median(ds_salaries$salary_in_usd)
quantile(ds_salaries$salary_in_usd)
```

we see that the mean is close to the median,and quantiles are close to one another.

```{r}
mean(ds_salaries$remote_ratio)
median(ds_salaries$remote_ratio)
quantile(ds_salaries$remote_ratio)
```

with only 3 valus for remote ratio we see a big diifrence between mean and median with the median being 0 wich means most work done was not onine and the quantile with the values 0 and 100 meaning there is really low number of work done half online half offline.

```{r}
mean(ds_salaries$work_year)
median(ds_salaries$work_year)
quantile(ds_salaries$work_year)
```

we see that the median is 2022 wich means that most pepole work year is 2022,and the quantile shows that the number of employee with work year 2020 are the least.

```{r}
var(ds_salaries$salary)
var(ds_salaries$salary_in_usd)
var(ds_salaries$remote_ratio)
var(ds_salaries$work_year)
```

we see that the var of salary is really high meaning higher risk var of salary in usd is also really high meaning higher risk the var of remote ratio is high but not quit high as salary and salary in usd meaning high risk work year var is low meaning lower risk

## changing the salary and salary in usd to be in 1000s

```{r}
ds_salaries$salary_1000s <- ds_salaries$salary /1000
head(ds_salaries$salary_1000s)

ds_salaries$salaryusd_1000s <- ds_salaries$salary_in_usd /1000
head(ds_salaries$salaryusd_1000s)
```

## graphs

we compared some variables with each other to find how they are distributed

#### boxplot:

```{r}
boxplot(ds_salaries$salary_1000s,ylim=c(0,1000),xlab="Salary",ylab="Salary in 1000s" ,main="boxplot of salary in 1000s")
```

Box plot represents salaries of employees,it displays the five number summary of the salary it shows that the salary has a lot of outliers that's need to be smoothed to remove the noise

```{r}
boxplot(ds_salaries$salaryusd_1000s,ylim=c(0,1000),xlab="Salary in USD",ylab="Salary in usd 1000s" ,main="boxplot of salary in usd in 1000s")
```

Box plot represents salary in (usd), it displays the five number summary of the salary in USD it shows that the salary in USD has a lot of outliers thats need to be smoothed to remove the noise

```{r}
boxplot(ds_salaries$work_year,xlab="Work year",ylab="Frequency"  ,main="boxplot of Work year")
```

Box plot represents work year (2020,2021,2022,2023), the work year box plot shows that there is a one outlier of 2020 year, that will be smoothed later to have more accurate data

### histogram

```{r}
hist(ds_salaries$salary_1000s,ylim=c(0,100),ylab="Frequency",xlab="salary in 1000s",main = "Salary Frequency")

```

The histogram represent the frequency of salaries for each employee, it shows that most of employees has small amount of salary

### bar plot

```{r}


ds_salaries$remote_ratio %>% table() %>% barplot(xlab="remote ratio", main="barplot of remote ratio")


ds_salaries$work_year %>% table() %>% barplot(xlab="work year", ylab="number of employees", main="barplot of work year")


  library(ggplot2)
library(tidyverse)
top_5_job_salaries<-ds_salaries%>%
  group_by(job_title)%>%
  summarise(Avg_Sal=mean(salary_1000s))%>%
  arrange(desc(Avg_Sal))%>%
  head()
top_5_job_salaries

```

First, the bar plot of 'remote ratio' represent the total of remote ratio for each employee, it show that most employees work on site then the employees who work online, the employees who work on both (online and onsite) have the smaller frequency

Second, the bar plot of 'work year' represent the work year and number of employee in each year, it show that 2020 year has the lowest number of employees, the number of employees increases annually

### Scatter plot

```{r}
  with(ds_salaries,plot(ds_salaries$salary_1000s,ds_salaries$salaryusd_1000s,xlab="salary in 1000s",ylab = "salary in usd 1000s",main="Scatter plot with salary and salary in USD") )
```

The scatter plot represents the correlation in salary and salary in usd, we notice that most of the salary and salary in usd are redundant data and the two attributes are strongly correlated

### pie chart

```{r}
library(dplyr)
ds_salaries2 <- ds_salaries %>% sample_n(50)

tab <- ds_salaries2$company_size%>% table()
precentages <- tab %>% prop.table() %>% round(3)*100
txt <- paste0(names(tab),'\n',precentages,'%')
pie(tab,labels=txt)

```

The pie chart represent the percentages for company size by taken sample of company, it shows that 2(Medium) has the highest frequency

### Visualisation

```{r}
ggplot(top_5_job_salaries, aes(x=job_title, y=Avg_Sal)) +
  geom_col() +
  labs(title="Top 5 Job Title Salaries", x="Job Title", y="Salary in 1000s") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

the bar plot of 'Top 5 job Title Salaries' represent the job title and the salary for each job, it shows that Head of Machine Learning has the highest salary

```{r}
ggplot(ds_salaries, aes(x=experience_level, y=salaryusd_1000s)) +
  geom_boxplot(fill="pink") +
  labs(title="Distribution of Salary by Experience Level", x="Experience Level", y="Salary in 1000s") +
  theme_minimal()
```

the box plot represent the percentages for experience level by taken sample of employees, it shows that salary aare different for each experience level, Also employees with the same experience level can have significantly higher salary than there category which can be observed from the box plot as outliers

## Average Salaries in each Year

```{r}
  library(dplyr)

yearly_salary_avg<-ds_salaries%>%
  group_by(work_year)%>%
  summarise(avg_salary=mean(salary_in_usd))
yearly_salary_avg
    
```

This table show the average salary increases annually for each year

## Data cleaning

### categorizing job title

Categorize job title column

```{r}
library(tidyverse)
categorize_job_title <- function(title) {
  title <- tolower(title)
  if (grepl("data scientist|research scientist|researcher|scientist|science", title)) {
    return("Data Scientist")
  } else if (grepl("data engineer|etl|machine learning software engineer|ai|data architect|engineer|data modeler|machine learning engineer|devops", title)) {
    return("Data Engineer")
  } else if (grepl("data analyst|bi|analyst|head of data|data lead|data strategist|data analytics|specialist|data manager", title)) {
    return("Data Analyst")
  } else {
    return("Other")
  }
}
ds_salaries <- ds_salaries %>% 
  mutate(job_title = sapply(job_title, categorize_job_title))
head(ds_salaries)
```

Our column job title had 93 different categories, By using this categorization process, the code assigns each job title in the dataset to a general job title, we categorized the job title into four categorize ("Data Scientist","Data Engineer","Data Analyst",Other) , allowing for easier analysis and grouping of data based on their job title.

### Categorizing company location:

```{r}


# Define the country categories
asia <- c("AE", "CN", "HK", "ID", "IN", "JP", "KR", "MY", "PH", "SG", "TH", "TW", "VN")
europe <- c("AL", "AM", "AT", "BA", "BE", "BG", "BY", "CH", "CZ", "DE", "DK", "EE", "ES", "FI", "FR", "GB", "GE", "GR", "HR", "HU", "IE", "IL", "IT", "LT", "LU", "LV", "MD", "MK", "MT", "NL", "NO", "PL", "PT", "RO", "RS", "RU", "SE", "SI", "SK", "TR", "UA", 'ES')

north_america <- c("CA", "US")
south_america <- c("AR", "BO", "BR", "BS", "CL", "CO", "CR", "DO", "EC", "GT", "HN", "JM", "MX", "NI", "PA", "PE", "PY", "SV", "UY", "VE")
oceania <- c("AS", "AU", "FJ", "GU", "KI", "MH", "MP", "NC", "NF", "NZ", "PG", "PW", "SB", "TO", "TV", "VU", "WS")
africa <- c("BF", "BI", "BJ", "BW", "CD", "CG", "CI", "CM", "CV", "DJ", "DZ", "EG", "ET", "GA", "GH", "GM", "GN", "GQ", "KE", "LR", "LS", "LY", "MA", "MG", "ML", "MR", "MU", "MW", "MZ", "NA", "NE", "NG", "RE", "RW", "SC", "SD", "SH", "SL", "SN", "SO", "SS", "ST", "SZ", "TD", "TG", "TN", "TZ", "UG", "ZA", "ZM", "ZW")

# Function to categorize the company locations
categorize_location <- function(location) {
  if (location %in% asia) {
    return("Asia")
  } else if (location %in% europe) {
    return("Europe")
  } else if (location %in% north_america) {
    return("North America")
  } else if (location %in% south_america) {
    return("South America")
  } else if (location %in% oceania) {
    return("Oceania")
  } else if (location %in% africa) {
    return("Africa")
  } else {
    return("Other")
  }
}

# Apply the categorization to the company location column
ds_salaries$company_location <- sapply(ds_salaries$company_location,categorize_location)

# Print the updated dataset with categories
tail(ds_salaries)
```

Our column company location did have 72 different categories, By using this categorization process, the code assigns each company location in the dataset to a specific geographical region, such as Asia, Europe, North America, South America, Oceania, and Africa, allowing for easier analysis and grouping of data based on different regions of the world.

### finding missing data

we used is null function to show any null values in the dataset.

```{r}
sum(is.na(ds_salaries))
```

we did not have any missing values.

### finding outliers:

By detecting outliers, analysts can identify potential errors or anomalies in the data collection process, data entry, or measurement inaccuracies

```{r}
boxplot.stats(ds_salaries$salary)$out
boxplot.stats(ds_salaries$salary_in_usd)$out              
boxplot.stats(ds_salaries$remote_ratio)$out
boxplot.stats(ds_salaries$work_year)$out
```

We did detect the outliers in our numeric attributes

### Sum outliers:

```{r}
sum(boxplot.stats(ds_salaries$salary)$out)

sum(boxplot.stats(ds_salaries$salary_in_usd)$out)

sum(boxplot.stats(ds_salaries$remote_ratio)$out)

sum(boxplot.stats(ds_salaries$work_year)$out)
```

### removing the outliers:

Outliers can sometimes distort our analysis and models, so by removing them, we get a clearer picture of the data and can make better predictions or understand patterns more accurately

```{r}


library(dplyr)

# Remove outliers for each variable
ds_salaries <- ds_salaries %>%
  filter(
    between(salary, quantile(salary, 0.25) - 1.5 * IQR(salary), quantile(salary, 0.75) + 1.5 * IQR(salary)),
    between(salary_in_usd, quantile(salary_in_usd, 0.25) - 1.5 * IQR(salary_in_usd), quantile(salary_in_usd, 0.75) + 1.5 * IQR(salary_in_usd)),
    between(work_year, quantile(work_year, 0.25) - 1.5 * IQR(work_year), quantile(work_year, 0.75) + 1.5 * IQR(work_year))
  )

ds_salaries <- ds_salaries %>%
  filter(
    between(salary, quantile(salary, 0.25) - 1.5 * IQR(salary), quantile(salary, 0.75) + 1.5 * IQR(salary)),
    between(salary_in_usd, quantile(salary_in_usd, 0.25) - 1.5 * IQR(salary_in_usd), quantile(salary_in_usd, 0.75) + 1.5 * IQR(salary_in_usd)),
    between(work_year, quantile(work_year, 0.25) - 1.5 * IQR(work_year), quantile(work_year, 0.75) + 1.5 * IQR(work_year))
  )
```

We did this to make sure our data is more accurate and representative of the majority of the values.

## Data transformation

### Normalization

Normalizing data helps in improving the performance of machine learning models,reducing the training time and improving efficiency.

```{r}
normalize <- function(x) {return((x-min(x))/(max(x)-min(x)))}
ds_salaries$salary<-normalize(ds_salaries$salary)
ds_salaries$salary_in_usd<-normalize(ds_salaries$salary_in_usd)

num1_cols=ds_salaries[, c(5,7 )] 
head(num1_cols)
```

we normalized the attributes salary, salary in usd, so it takes values between 0 and 1 ,which helps in handling the data

### Discretization

transforming continuous data into categorical values.

```{r}


breaks <- quantile(ds_salaries$salary_in_usd, 
                   probs = c(0, 1/3, 2/3,1), 
                   na.rm = TRUE)

ds_salaries$salary_in_usd_disc <- cut(ds_salaries$salary_in_usd, 
                             breaks = breaks, 
                             include.lowest = TRUE, 
                             labels=c("low", "mid", "high"))
head(ds_salaries$salary_in_usd_disc)
                               
```

Discretization makes it easier to analyze and interpret the data

Encoding onverting categorical or ordinal variables into a numerical representation that can be used in data analysis and modeling. This transformation is necessary because many machine learning algorithms and statistical techniques require numerical inputs

### Encoding

```{r}
ds_salaries$company_size = factor(ds_salaries$company_size,levels = c("S","M","L"),labels = c(1,2,3))
ds_salaries$experience_level = factor(ds_salaries$experience_level,levels = c("EN","MI","SE","EX"),labels = c(1,2,3,4))

ds_salaries$job_title  <- factor(ds_salaries$job_title)
ds_salaries$employment_type  <- factor(ds_salaries$employment_type)
ds_salaries$employee_residence  <- factor(ds_salaries$employee_residence)
ds_salaries$company_location  <- factor(ds_salaries$company_location)
ds_salaries$salary_currency  <- factor(ds_salaries$salary_currency)


num_cols=ds_salaries[, c(2,11 )]
head(num_cols)
```

we encoded our ordinal and nominal variables using factor

## Removing irrelevant and duplicate attributes from the dataset

our data set has 2 salary coulombs salary, and salary in usd, for this, we will measure the correlation between them

```{r}

cor(ds_salaries$salary,ds_salaries$salary_in_usd)
```

we see that salary and salary_in_usd are highly correlated so we will only take the salary in USD.

we decided to remove employee resident due to its being irrelevant to our data sense the employee resident has nothing to do with his salary.

we also decided to remove salary currency sense we only took salary in USD.

```{r}
library(tidyverse)
ds_salaries <- ds_salaries %>%
  select(salary_in_usd_disc,work_year,experience_level,employment_type,job_title,company_location,company_size,remote_ratio)
  head(ds_salaries)
```

# Classification

## checking if the data is balanced

```{r}
barplot(prop.table(table(ds_salaries$salary_in_usd_disc)),
        col = rainbow(2),
        ylim = c(0, 0.7),
        main = "Class Distribution")
```

as shown in the bar plot above, our class lable is balanced, so we can split our data randomly

## Split the datasets into two subsets: Training(80%) and Testing(20%):





```{r}
library(caret)
set.seed(2021)
create_train_test <- function(data, size = 0.8, train = TRUE) {
    n_row = nrow(data)
    total_row = size * n_row
    train_sample <- 1: total_row
    if (train == TRUE) {
        return (data[train_sample, ])
    } else {
        return (data[-train_sample, ])
    }
}
```

```{r}
data_train <- create_train_test(ds_salaries, 0.8, train = TRUE)
data_test <- create_train_test(ds_salaries, 0.8, train = FALSE)
dim(data_train)
```

```{r}
dim(data_test)
```

The train dataset has 2840 rows while the test dataset has 711 rows.

we use the function prop.table() combined with table() to verify if the randomization process is correct.

```{r}
prop.table(table(data_train$salary_in_usd_disc))
```

```{r}
prop.table(table(data_test$salary_in_usd_disc))
```


### gini index

```{r}
library(rpart)
library(rpart.plot)
fit <- rpart(salary_in_usd_disc~., data = data_train, method = 'class')
rpart.plot(fit, extra = 100,type = 4)

```

rpart() function uses the Gini impurity measure to split the note.

```{r}
predict_unseen <-predict(fit, data_test, type = 'class')
```





```{r}
result<-table(predict_unseen, data_test$salary_in_usd_disc)
co_result <- confusionMatrix(result)
print(co_result)
```

### gain ratio



```{r}
library(C50)
library(printr)
library(caret)

```

Train decision tree

```{r}
model <- C5.0(salary_in_usd_disc ~., data=data_train)
```

Test

```{r}
results <- predict(model, data_test, type="class")

```

```{r}

plot(model, type="simple")
```

```{r}
library(caret)
result1<- table(results, data_test$salary_in_usd_disc)
co_result1 <- confusionMatrix(result1)
print(co_result1)
```

### information gain

```{r}
classify_dataset <- function(dataset, class_label) {
  # Load the required libraries
  library(rpart)
  library(rpart.plot)
  library(caret)

 


  # Create the decision tree model using information gain
  decision_tree <- rpart(formula = as.formula(paste(class_label, "~ .")), data = data_train, method = "class", control = rpart.control(cp = 0))

  # Plot the decision tree with reduced text and symbol size
  rpart.plot(decision_tree, extra = 100, cex = 0.5)
  
 # Predict on the test set
  predictions <- predict(decision_tree, newdata = data_test, type = "class")
  
  # Create confusion matrix
  confusion_matrix <- confusionMatrix(predictions, data_test[, class_label])
  
  # Print confusion matrix and statistics
  print(confusion_matrix)

}

classify_dataset(ds_salaries, "salary_in_usd_disc")

```


# 2-Split the Dataset into two subsets: Training(70%) and Testing(30%):


```{r}
set.seed(132)

# Generate indices for the data partition
train_indices_1 <- sample (2, nrow(ds_salaries), replace=TRUE, prob=c(0.7 , 0.3))

# Create the training and test sets
data_train70 <- ds_salaries[train_indices_1==1, ]
data_test70 <- ds_salaries[train_indices_1==2, ]

```





```{r}
dim(data_train70)
```


```{r}
dim(data_test70)
```


# The train set has 2498 rows while the test set has 1049 rows.

###A-Decision Tree Using gini index

```{r}
library(rpart)
library(rpart.plot)
fit <- rpart(salary_in_usd_disc~., data = data_train70, method = 'class')
rpart.plot(fit, extra = 100,type = 4)
```

# rpart() function uses the Gini impurity measure to split the note.



```{r}
predict_unseen70 <-predict(fit, data_test70, type = 'class')
table_mat70 <- table(data_test70$salary_in_usd_disc, predict_unseen70)
table_mat70
```
```{r}
accuracy_Test70 <- sum(diag(table_mat70)) / sum(table_mat70)
```

```{r}
print(paste('Accuracy for test', accuracy_Test70))
```


```{r}


result70_gini<-table(predict_unseen70, data_test70$salary_in_usd_disc)
co_result70_gini <- confusionMatrix(result70_gini)
print(co_result70_gini)
```



###B-Decision Tree Using gain ratio


#Train decision tree

```{r}
library(C50)
library(printr)
model70 <- C5.0(salary_in_usd_disc ~., data=data_train70)
```

# Make predictions on the test data

```{r}
results70_gain <- predict(object=model70, newdata=data_test70, type="class")

```


```{r}
plot(model70)
```


#Confusion Matrix and Statistics

```{r}
result70<-table(results70_gain, data_test70$salary_in_usd_disc)
co_result70_gain <- confusionMatrix(result70)
print(co_result70_gain)
```


###C-Decision Tree Using information gain

```{r}
classify_dataset <- function(dataset, class_label) {



decision_tree <- rpart(formula = as.formula(paste(class_label, "~ .")), data = data_train70, method = "class", control = rpart.control(cp = 0))


  rpart.plot(decision_tree, extra = 100, cex = 0.5)
 
  predictions <- predict(decision_tree, newdata = data_test70, type = "class")

  # Create confusion matrix
  confusion_matrix <- confusionMatrix(predictions, data_test70[, class_label])
  
  # Print confusion matrix and statistics
  print(confusion_matrix)
}

classify_dataset(ds_salaries, "salary_in_usd_disc")
```


## Split the datasets into two subsets: Training(90%) and Testing(10%):





```{r}
library(caret)
set.seed(2021)
create_train_test <- function(data, size = 0.9, train = TRUE) {
    n_row = nrow(data)
    total_row = size * n_row
    train_sample <- 1: total_row
    if (train == TRUE) {
        return (data[train_sample, ])
    } else {
        return (data[-train_sample, ])
    }
}
```

```{r}
data_train <- create_train_test(ds_salaries, 0.9, train = TRUE)
data_test <- create_train_test(ds_salaries, 0.9, train = FALSE)
dim(data_train)
```

```{r}
dim(data_test)
```

The train dataset has 2840 rows while the test dataset has 711 rows.

we use the function prop.table() combined with table() to verify if the randomization process is correct.

```{r}
prop.table(table(data_train$salary_in_usd_disc))
```

```{r}
prop.table(table(data_test$salary_in_usd_disc))
```


### gini index

```{r}
library(rpart)
library(rpart.plot)
fit <- rpart(salary_in_usd_disc~., data = data_train, method = 'class')
rpart.plot(fit, extra = 100,type = 4)

```


```{r}
predict_unseen <-predict(fit, data_test, type = 'class')
```





```{r}
result<-table(predict_unseen, data_test$salary_in_usd_disc)
co_result <- confusionMatrix(result)
print(co_result)
```

### gain ratio



```{r}
library(C50)
library(printr)
library(caret)

```

Train decision tree

```{r}
model <- C5.0(salary_in_usd_disc ~., data=data_train)
```

Test

```{r}
results <- predict(model, data_test, type="class")

```

```{r}

plot(model, type="simple")
```

```{r}
library(caret)
result1<- table(results, data_test$salary_in_usd_disc)
co_result1 <- confusionMatrix(result1)
print(co_result1)

```

### information gain

```{r}
classify_dataset <- function(dataset, class_label) {
  # Load the required libraries
  library(rpart)
  library(rpart.plot)
  library(caret)

 


  # Create the decision tree model using information gain
  decision_tree <- rpart(formula = as.formula(paste(class_label, "~ .")), data = data_train, method = "class", control = rpart.control(cp = 0))

  # Plot the decision tree with reduced text and symbol size
  rpart.plot(decision_tree, extra = 100, cex = 0.5)
  
 # Predict on the test set
  predictions <- predict(decision_tree, newdata = data_test, type = "class")
  
  # Create confusion matrix
  confusion_matrix <- confusionMatrix(predictions, data_test[, class_label])
  
  # Print confusion matrix and statistics
  print(confusion_matrix)

}

classify_dataset(ds_salaries, "salary_in_usd_disc")

```

